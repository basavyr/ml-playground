# Description of ViT

## Observations

Setting `dynamic_img_pad` to True will make sure that the model will work on the input image even if the (H,W) (i.e., `img_size`) is not divisible by the patch size P
For example, if training MNIST-like data (28x28 pixels) and setting P=8, then we see the following error when trying to perform training on the model:
```bash
AssertionError: Input height (28) should be divisible by patch size (8).
```

By putting:
```python
vit = VisionTransformer(img_size=data_config.img_size,
                            patch_size=patch_size,
                            in_channels=data_config.in_channels,
                            num_classes=data_config.num_classes,
                            dynamic_img_size=True,
                            dynamic_img_pad=True)
```
will make the model work for any arbitrary patch size.

## Dynamic Image Padding & Sequence Length
When `dynamic_img_pad=True` is used, the input image is padded to the nearest dimensions divisible by the `patch_size`. 

**Example (MNIST 28x28, Patch Size 8):**
- **Padding:** The 28x28 image is padded to **32x32**.
- **Grid Size:** This results in a **4x4** grid of patches ($32/8 = 4$).
- **Number of Patches:** $4 \times 4 = 16$ patches.
- **Sequence Length:** Sum of patches and prefix tokens (e.g., $16 + 1 \text{ (CLS token)} = 17$).

**Impact on Model Components:**
- **Embedding Dimension (`embed_dim`):** **Unchanged**. This is a fixed architectural parameter that defines the "width" of each token.
- **Number of Embeddings:** The **sequence length** (number of tokens) changes to match the number of patches generated from the padded image.
- **Positional Embeddings:** Since the number of patches changed, the model must **interpolate/resample** the learnable positional embeddings to match the new grid size ($4 \times 4$ in this case). This is handled by the `resample_abs_pos_embed` function using bicubic interpolation.

## Positional Embedding Resampling (`resample_abs_pos_embed`)

The `resample_abs_pos_embed` function is critical for maintaining spatial consistency when the image resolution or patch size changes. In a standard ViT, positional embeddings are a fixed-size learnable parameter. If the number of patches changes, these embeddings must be adjusted to match the new grid.

### Calling Circumstances

1.  **On-the-fly Inference (`_pos_embed`):**
    *   **When:** `dynamic_img_size` is set to `True`.
    *   **Why:** If an input image is padded (e.g., via `dynamic_img_pad`) or provided at a different resolution, the number of tokens generated by the patch embedding layer will differ from the model's pre-defined `pos_embed` length.
    *   **Effect:** The model interpolates the positional embeddings specifically for that forward pass, allowing it to "understand" the spatial layout of the new grid.

2.  **Model Reconfiguration (`set_input_size`):**
    *   **When:** The user explicitly calls `model.set_input_size(...)`.
    *   **Why:** To permanently update the model's base configuration (e.g., when transitioning from 224x224 training to 384x384 fine-tuning).
    *   **Effect:** The `pos_embed` parameter is resampled and replaced, ensuring all future forward passes use the new grid size by default.

### Technical Logic
*   **Prefix Preservation:** Tokens like the `CLS` or `register` tokens are stripped before resampling. They do not have spatial coordinates and are concatenated back unchanged after the process.
*   **2D Interpolation:** The remaining 1D sequence of spatial embeddings is reshaped into a 2D grid ($H_{old} \times W_{old}$).
*   **Bicubic Scaling:** The grid is scaled to the new dimensions ($H_{new} \times W_{new}$) using bicubic interpolation. This allows the model to map its learned "sense of space" from a low-resolution grid to a high-resolution one (or a padded one).
*   **Flattening:** The interpolated grid is flattened back into a 1D sequence to be added to the patch tokens.

## Sequence Length Check

In the `set_input_size` method, the model explicitly checks if the positional embeddings need to be updated by calculating the **target sequence length**:

```python
num_new_tokens = self.patch_embed.num_patches + num_prefix_tokens
if num_new_tokens != self.pos_embed.shape[1]:
    # Trigger Resampling
```

*   **`num_new_tokens`**: The sum of spatial patches and prefix tokens (CLS, registers).
*   **Validation**: If the calculated length doesn't match the second dimension of the current `pos_embed` tensor, it confirms that the image/patch configuration has shifted, and the positional embeddings are no longer compatible and must be resampled.
